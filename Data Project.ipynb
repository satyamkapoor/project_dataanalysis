{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jeroen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jeroen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "starting for loop!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "from csv_helper import CSVHelper\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "data = CSVHelper.load_csv(\"Tweets_2016London.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(tweets):\n",
    "    tknzr = TweetTokenizer()\n",
    "    tokenized_tweets = []\n",
    "    for tweet in tweets:\n",
    "        tokenized_tweets.append(tknzr.tokenize(tweet))\n",
    "    return tokenized_tweets\n",
    "\n",
    "def remove_stopwords(tweets):\n",
    "    stopwords =  nltk.corpus.stopwords.words('english')\n",
    "    tweets_nostop = []\n",
    "    for tweet in tweets:\n",
    "        tweet_nostop = [w.lower() for w in tweet if w.lower() not in stopwords]\n",
    "        tweets_nostop.append(tweet_nostop)\n",
    "    return tweets_nostop\n",
    "\n",
    "def filter_noise(tweets):\n",
    "    tweets_filtered = []\n",
    "    for tweet in tweets:\n",
    "        tweet_filtered = [re.sub(\"[^a-zA-Z0-9]\",'', w) for w in tweet if re.sub(\"[^a-zA-Z0-9]\",'', w)!= '']\n",
    "        tweets_filtered.append(tweet_filtered)\n",
    "    return tweets_filtered\n",
    "\n",
    "def remove_url(tweets):\n",
    "    tweets_filtered = []\n",
    "    for tweet in tweets:\n",
    "        tweet_filtered = [re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', w) for w in tweet]\n",
    "        tweets_filtered.append(tweet_filtered)\n",
    "    return tweets_filtered\n",
    "\n",
    "#we decided to use a lemmatizer instead of stemming \n",
    "def my_stem(tweets):\n",
    "    lmt = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    tweets_stemmed = []\n",
    "    for tweet in tweets:\n",
    "        tweet_stemmed = [lmt.lemmatize(w) for w in tweet]\n",
    "        tweets_stemmed.append(tweet_stemmed)\n",
    "    return tweets_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = tokenize(data)\n",
    "all_tweets = remove_url(all_tweets)\n",
    "all_tweets = filter_noise(all_tweets)\n",
    "all_tweets = remove_stopwords(all_tweets)\n",
    "all_tweets = my_stem(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function create_tfidf_features  (input = array of tokenized tweets output = tf-idf feature matrix) Creates a distinct vocabulary set, and a dictionary (key val pairs)for each word in the vocabulary, calculated the IDF value, and for each term in each tweet, calculates the tfidf score (which is just the term frequency * IDF value)\n",
    "\n",
    "\n",
    "https://nlpforhackers.io/tf-idf/\n",
    "\n",
    "https://www.youtube.com/watch?v=4vT4fzjkGCQ&t=1s\n",
    "\n",
    "http://billchambers.me/tutorials/2014/12/21/tf-idf-explained-in-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def termfreq(word, tweet):\n",
    "    #print(\"tweet:\", doc, \"length of tweet:\", len(doc))\n",
    "    num_words = len(tweet)\n",
    "    word_occurences = tweet.count(word)\n",
    "    return word_occurences/num_words\n",
    "\n",
    "def count_doc_with_term(word, tweets):\n",
    "    counter = 0\n",
    "    for tweet in tweets:\n",
    "        if word in tweet:\n",
    "            counter = counter + 1\n",
    "    return counter\n",
    "\n",
    "#idf = amount_of_tweets/amount_of_tweets_with_word\n",
    "def idf(word, tweets):\n",
    "    num_tweets = len(tweets)\n",
    "    num_tweets_with_word = count_doc_with_term(word, tweets)\n",
    "    return np.log10(num_tweets/num_tweets_with_word)\n",
    "\n",
    "def create_vocabulary(tweets):\n",
    "    vocabulary = set()\n",
    "    for tweet in tweets:\n",
    "        words = [w for w in tweet]\n",
    "        vocabulary.update(words)\n",
    "    return list(vocabulary)\n",
    "\n",
    "def compute_word_idf(tweets, vocabulary):\n",
    "    word_idf = collections.defaultdict(lambda: 0)      \n",
    "    for w in vocabulary:\n",
    "        word_idf[w] = idf(w, tweets)\n",
    "    return word_idf\n",
    "    \n",
    "def tf_idf(word, tweet, tweets):\n",
    "    if tweet != []:\n",
    "        return termfreq(word, tweet) * word_idf[word]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def tweet_tfidf_features(tweets):\n",
    "    vocabulary = create_vocabulary(tweets)\n",
    "    word_idf = compute_word_idf(tweets, vocabulary)\n",
    "    tweet_features = np.empty(len(tweets), dtype=object)\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        a = np.zeros(len(vocabulary))\n",
    "        for j, word in enumerate(vocabulary):\n",
    "            a[j] = tf_idf(word, tweet, tweets)\n",
    "        tweet_features[i] = a\n",
    "    return np.vstack(tweet_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test = [[\"this\",\"a\",\"a\",\"is\",\"sample\"],[\"this\",\"another\",\"another\",\"is\",\"example\",\"example\",\"example\"]]\n",
    "#test2 = [[\"new\",\"times\",\"york\"],[\"new\",\"post\",\"york\"],[\"angeles\",\"los\",\"times\"]]\n",
    "feature_matrix = tweet_tfidf_features(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_distance(v1,v2):\n",
    "    nomer = np.dot(v1, v2)\n",
    "    denomer = np.dot(np.linalg.norm(v1), np.linalg.norm(v2))\n",
    "    return nomer/denomer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO from here\n",
    "def calculate_distances(features):\n",
    "    size = len(features)\n",
    "    distances = np.zeros((size, size))\n",
    "    for i in range(len(distances)):\n",
    "        for j in range(len(distances)):\n",
    "            distances[i,j] = cosine_distance(features[i], features[j])\n",
    "    return distances\n",
    "\n",
    "#distances = calculate_distances(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
